\documentclass{article}

\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{color}

\definecolor{mygreen}{rgb}{0,0.6,0} 
\definecolor{mygray}{rgb}{0.5,0.5,0.5} 
\definecolor{mymauve}{rgb}{0.58,0,0.82}

%Scala
\lstdefinelanguage{scala}{
  morekeywords={abstract,case,catch,class,def,%
    do,else,extends,false,final,finally,%
    for,if,implicit,import,match,mixin,%
    new,null,object,override,package,%
    private,protected,requires,return,sealed,%
    super,this,throw,trait,true,try,%
    type,val,var,while,yield},
  otherkeywords={=>,<-,<\%,<:,>:,\#,@},
  sensitive=true,
  morecomment=[l]{//},
  morecomment=[n]{/*}{*/},
  morestring=[b]",
  morestring=[b]',
  morestring=[b]"""
}

% Default settings for code listings
\lstset{frame=tb,
  language=scala,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{mygray},
  keywordstyle=\color{blue},
  commentstyle=\color{mygreen},
  stringstyle=\color{mymauve},
  frame=single,
  breaklines=true,
  breakatwhitespace=true
  tabsize=3
}

\title{Metrics for code 'functional-ness' in Scala\\Dissertation proposal  }
\date{\today}

\begin{document}



\pagenumbering{gobble}
\maketitle

\vspace{30mm} %5mm vertical space

\begin{center}
Master Student Name: Alexandru Matei \par
Supervisor Name : Marius Minea \par
Dissertation Domain: Software engineering - Functional Programming
\end{center}

\vfill

\newpage
\tableofcontents
\newpage

\pagenumbering{arabic}
\section{Introduction}
Functional programming has started to gain more traction in the last years, thanks to the growing adoption of Scala and Haskell in the software industry; we can see that Google trends shows a rising interest in functional programming languages (see figure \ref{fig:google-rank}). \par

\begin{figure}[h!]
  \includegraphics[width=\linewidth]{google-trends.png}
  \caption{Google search hits for programming languages - 2004 to July, 2015 }
  \label{fig:google-rank}
\end{figure}

Scala is one of the leading functional programming languages, over the last couple of years being adopted by large companies such as  Twitter(2009), Linkedin(2010), The Guardian, Foursquare. If we look at 'RedMonk Programming Languages Rankings'\cite{redmonk:1}, which is based on StackOverflow and Github analysis,  Scala occupies a worthy 14th position, being the first FP language in the top . \par

\begin{figure}[h!]
  \includegraphics[width=\linewidth]{redmonk-rank.png}
  \caption{Redmonk rank of programming languages - June, 2015}
  \label{fig:redmonk-rank}
\end{figure}

One of its success factors is  Scala's compatibility with Java Virtual Machine hence code interoperability with Java, and the benefits that come along from  Java's well-established ecosystem.\par

 Another one might be that Massive Open Online Courses (MOOCs)  like Functional Programming in Scala \cite{scalastat:1} held  by the creator of Scala, Martin Odersky and Reactive Programming are one of the most popular online courses; the feedback from developers is really encouraging(see figure \ref{fig:interest}). \par

\begin{figure}[h!]
  \includegraphics[width=\linewidth]{interest.png}
  \caption{Feedback FP in Scala course - interest in future courses on Scala (FP)}
  \label{fig:interest}
\end{figure}

Another reason why functional programming comes in handy nowadays is that multiprocessor architectures become ubiquitous and developers need to get more familiar with distributed and multi-threaded computing. Programming with shared state has proven to be  difficult to reason about and functional programming offers to save a lot of headaches by removing shared state from the equation and providing better concurrency abstractions (Futures, Actors). We should also consider  the exceptional distribution capabilities of map-reduce that functional programming offers. Although the programming paradigm was born more than 80 years ago, functional programming seems like a totally new way of thinking about software to modern day developers, having lots of secret gems left to be discovered. \par

\section {Problem statement}
Scala is a hybrid programming lanaguage combining both  functional and  object oriented elements \cite{scalalang:1}. Writing Scala code does not imply you are writing functional code;  there is no mechanism in the language that enforces a functional style of programming; so one can say they write code in Scala so they use functional programming when in fact their code base is entirely procedural. I was also put in such a position,  being one of the many  developers that switched from an Object Oriented language(Java /C\#/C++xs etc.)  to Scala. So how can one figure out if his Scala code follows the functional programming principles or not? The classic approach is to ask a functional programming expert for code review, if you are lucky enough to have access to such valuable resources. \par

The solution we propose is a set of metrics that should be able to quantify to what degree the code is making use of the functional paradigm. But what  qualifies a code as being functional? What are the characteristics of functional code? In the following section we will elaborate the main characteristics of functional programming languages.\par

\section {Theoretical Foundations}
First of all we need to establish the main elements of functional languages and we will do so by taking Scala as an example. Only then we can elaborate further on the metrics and decide which ones are to be considered for the study. \par


The question we will try to answer in this body of work is 'how much does a method use functional programming concepts?' The main elements we will look at are: immutability, referential trasparency, high-order functions, monads, laziness.

%We get to ask ourselves the following questions: `how much does a method uses functional programming concepts?` Does it contain only immutable data, so no shared state? Does it use higher order functions for data processing? Does a method use advanced features such as monadic comprehension? Does a method use functional-like constructs such as anonymous inner classes? \par

\subsection {Immutability}
If a variable/object is immutable then its value never changes. That is a useful guarantee if one plans on going concurrent: any such value can be safely shared amongst treads since the value is read-only. Another advantage is the reduced aliasing between different parts of the program; one can say that immutable objects are easier to reason about and also their API is straight forward because you don't need to reason about internal state changes - everything is transparent compared to mutable objects that introduce opaqueness in reasoning about their possible states at different moments of time . \par
Scala encourages immutability through case classes, which provide a syntatic sugar for creating immutable objects (data structures). Case classes are regular classes which export their constructor parameters and which provide a recursive decomposition mechanism via pattern matching.

\begin{lstlisting} 
abstract class Pet
case class Dog(name:String) extends Pet 
case class Cat(name:String) extends Pet
case class Hippo(name:String, weight:Int) extends Pet

//Decomposition
def printPetType(pet:Pet): String= pet match{
  case Dog => "it's a Dog"
  case Cat => "it's a Cat"
  case Hippo => "it's a Hippo"
}
\end{lstlisting}

By default, case classes ar immutable but one can change some of fields to be mutable, although it is not recommended.\par

Declaring a variable as a 'val' prevents it for being reassigned; still, this doesn't guarantee that the object it is reffering to is immutable. Best transparency is obtained with using both val's and immutable objects: this is the recommended approach also when dealing with concurrency. Scala provides both mutable and immutable collections that can be found in packages scala.collection.mutable and scala.collection.immmutable repectively. By default, Scala always picks immutable collections. 

\begin{lstlisting} 
val x = 4
x = 5 //Reasignment to val generates error
\end{lstlisting}


So, in order to create an immutable Scala object, it is necessary to have all the fields declared as vals + all the values to be immutable objects in turn. Case classes make it easier to accomplish that in as few lines of code as possible.

\subsection {Referential tranparency, pure functions}
An expression is referentially transparent (RT) if it can be replaced by its resulting value without changing the behavior of the program. This must be true regardless of where the expression is used in the program. Programming without side effects leads to referential transparency. An example:

\begin{lstlisting}
def f(x:Int)= x* 3
val z = f(3)
\\Now, whenever we use z in the code, we can safely
\\ replace it with f(3) without changing the result of the program
\end{lstlisting}

Pure functions evaluate to the same result given the same argument value(s) and  don't have any side effects. A definition combining both concepts can be found in 'Functional Programming in Scala' by Chiusano and Bjarnason : 'A function f is pure if expression f(x) is referentially transparent for all referentially transparent values x'.\par
Examples of pure functions in Scala include:
\begin{itemize}

 \item Methods on immutable collections such as map, drop, filter, take
 \item Methods like split, length on the String class
 \item Mathematical functions such as add, multiply \ldots
\end{itemize}
As a rule of thumb in Scala if a function has return type Unit, then most probably it has side effects and it is not pure. \par

A measure for the degree of referential transparency in a program can be expressed as:  \par
\begin{center}
\begin{math}
  RTI = \frac{ | \{ m \in M / Pure(m) \} | } { | M | }
\end{math}
\end{center}

Where RTI- Referential Transparency Index ,  M - set of all functions/methods of the software and  Pure(m) - predicate that says if a function is pure or not. In this case, a value close to 1 corresponds to improved referential transparency while one close to 0 says that the property is not satisfied at all. \cite{DBLP:conf/icse/MudduABP13}. Further study is needed on elaborating how the pure function should be defined. \par

\subsection {High-order functions}
The central concept of functional languages as pointed out by John Hughes \cite{DBLP:journals/cj/Hughes89} in 'Why Functional Programming Matters' is higher- order functions ; Hughes argues that when high order functions are combined with laziness techniques they greatly increase the modularity of software by providing novative ways (compared to other structured programming techniques) of 'gluing' modules together so programs can become more concise and easier to reason about. 

Before we begin talking about high-order functions we should first understand what does an order of a function mean:

\begin{itemize}
\item Order 0: Non function data
\item Order 1: Functions with domain and range of order 0
\item Order 2: Functions with domain and range of order 1
\item Order k: Functions with domain and range of order k-1
\end{itemize}

So order 0 is represented by numbers, lists, characters, etc. Order 1 are functions wich work with order 0 data. So order 1 data are the well known functions that every programming language supports. 
Functions with an order grater than 1 are callled higher-order functions and they fall at least in on of the following categories:
\begin{itemize}
\item they take other functions as parameters
\item they return a function as a result
\end{itemize}

Being able to pass functions as parameters and return them as results means that functions are first-class citizens and is one of the  core concepts that make a functional language.

An example is the following apply function written in Scala, which takes a function  and an integer as parameters and produces  a function as a result: \par

\begin{lstlisting} 
def apply(f: Int => String, v: Int) = f(v)
\end{lstlisting} 


Classical higher-order functions over lists :

\begin{itemize}
\item Mapping: Application of a function on all elements in a list
\item Filtering : Collection of elements from a list which satisfy a particular condition
\item Accumulation: Pair wise combination of the elements of a list to a value of
another type
\item Folding: Reducing a list over some function with accumulator
\end{itemize}
[http://people.cs.aau.dk/~normark/prog3-03/pdf/higher-order-fu.pdf]

A study about high-order functions metrics and their corelation with software modularity was also conducted by B. Muddu et. al. \cite{DBLP:conf/icse/MudduABP13}. The proposed metric was studying the coupling between high order functions in diferent modules. It might be useful to make use also of their modularity metric when assesing code functional-ness.\par 

\subsection {Monads}  \label{monads}
Monads are a central design pattern of functional programming. They've been successfully used to abstract over well known problems in programming \cite{Jones01tacklingthe}: non-determinism as expressed by list, time/ concurrency, mutable state (see IO Monad Haskell), exception handling, foreign language calls.

In Category Theory, a Monad is a functor equipped with a pair of natural transformations satisfying the laws of associativity and identity. In Scala, monads are just a parametric type M[T] with two operations, flatMap (or bind) and unit which also preserves associativity and identity. \par

\begin{lstlisting}
trait M[T]{
  def flatMap[U](f:T=>M[U]): M[U]
}

def unit[T](x:T): M[T]
\end{lstlisting}

\subsection {Lazy evaluation}
Lazy evaluation or call-by-need is the opposite of eager evaluation(call-by-value) and it is an evaluation strategy which delays the evaluation of an expression until it is needed and only then computes the result and caches it for further evaluations. \par
One advantage is the memory saving due to postponing the evaluation but it comes with a performance cost: you get faster intialization but later (when evaluation occurs) you suffer a performance penality.\par

Scala supports laziness by introducing the lazy keyword and call by name parameters. By default it supports strict evaluation in contrast with Haskell which is lazy by default.

\begin{lstlisting}
lazy val product = 100 * 30 // not evaluated
println(product.toString) // evaluated 


object Test {
   def main(args: Array[String]) {
        delayed(time());
   }

   def time() = {
      println("Getting time in nano seconds")
      System.nanoTime
   }
   def delayed( t: => Long ) = {
      println("In delayed method")
      println("Param: " + t)
      t
   }
}

\end{lstlisting}

\section {State of the art - Literature study}
In the first section we talked about elaborating a series of metrics, in order to asses one's code functional-ness but we didn't have the opportunity to provide a proper definition for the concept of a metric and it's existing state of affairs in software industry. An easy definiton would be that metrics offer a quantitative measure of a software property. 

\subsection{Metrics overview and their use in FP languages}

\textit{"When you can measure what you are speaking about and express it in numbers, you know something about it; but when you cannot measure it, when you cannot express it in numbers, your knowledge is of a meagre and unsatisfactory kind: it may be the beginnings of knowledge but you have scarcely in your thoughts advanced to the stage of Science." (Lord Kelvin)}\par

A metric is a measurement function, and a software quality metric is "a function whose inputs are software data and whose output is a single numerical value that can be interpreted as the degree to which software possesses a given attribute that affects its quality \cite{Kaner04softwareengineering}. To some extent, metrics could be regarded as compiler warnings, signaling that a part of your code needs more testing. Software metrics are used in Software Engineering for helping with software development activities such as testing and refactoring, performance optimization, debugging, cost estimation, etc. Some common software metrics include : source line of code (LOC), number of functions, cyclomatic complexity, code coverage, cohesion, coupling. \par

Study of software metrics has been an active area of research since early 70' targeting mostly object oriented and imperative languages \cite{RyderT05:TFP_2005_Intellect}; as for functional languages, the number of published papers is not the numoerous; some of the prior work on metrics for functional languages was started almost 20 years ago by K. van den Berg in 1995 \cite{DBLP:journals/infsof/BergB95} who proposed a set of metrics for evaluating code complexity of Miranda functional programming language and Harrison which studied code modularity for SML \cite{eps250597}; 10 years later, Ryder and Thompson proposed some metrics for Haskell \cite{RyderT05:TFP_2005_Intellect} whereas  Muddu et. al. developed metrics for Scala \cite{DBLP:conf/icse/MudduABP13}. We can corelate the few number of papers on FP with the fact that FP does have yet such large adoption in software industry compared to object oriented and imperative languages. So we can devide prior work on metrics into flow-graph metrics and functional-related metrics: referential transparency, high-order functions, pure functions\cite{DBLP:conf/icse/MudduABP13} \par

Most commonly used metrics for functional languages in the papers we mentioned about are:

\begin{itemize}
\item Pattern related metrics - applies mostly to Haskell
\item Scoping metrics -how many scopes does a function introduce, the number of declatations brought in scope
\item Call graph metrics - strong connections, in degree, out degree, depth, width, arc-to-node ratio
\item Function atributes - path count, number of operands vs operators
\end{itemize}

In oder to study the maintability, Basavaraju Muddu et. al. introduce a technique for breaking software into logical modules \cite{DBLP:conf/icse/MudduABP13} The technique they chose was Modularizing by  Inverse-Depth heuristic, which is based of the project's file directory structure; they start at the source root folder and compute for each branch the maximum depth; then starting from the bottom up, they assign a module up to a certain depth; in their study, they use inverse depth of 1 for selecting modules, arguing that these are the closest approximation to actual logical modules. We would like to try other modularization techniques: package based, SBT (Scala Build Tool)  modularization and relate them to our problem: establishing code functional-ness: would a good 'functional-ness' value for our proposed metric lead also to improved modularity?.\par

The biggest problem with code metrics is not the definition of new ones but the extraction of  meaningful information from them;code metrics without semantics attached to them are just plain numbers; we need to corelate them to some process development metrics and this is the hardest part, since process activities are difficult to cuantify; most of the research in FP metrics is targeting maintanability, code quality. We would like to see how functional property metrics impact refactorings; is there a way to detect 'functional code smells'?.  \par

Given previous papers on measuring programs written in functional languages, we would like first to find means to measure (if possible) all functional properties described in the previous chapter and try to relate them to both code quality and possible refactorings; it would be nice to study if some object-oriented structres could be restructred to FP patterns. Huiqing Li propose a refactoring tool for functional programms written in Haskell\cite{Li:2003:TSR:871895.871899}; starting from here, we can imagine corelating our metrics with possible refactorings suggestions, of course, this goal is rather ambitious but it's worth a little more study on the matter.

\subsection {Combining Functional and Imperative Programming for Multicore Software: An Empirical Study Evaluating Scala and Java}
Pankratius et. al. performed a comparison study on Scala and Java's parallel programming features and performance \cite{Pankratius_combiningfunctional}. Their study shows that writing Scala code results in fewer lines of code than in Java and it preserves the application performance overall. One problem they notice is that Scala's support for parallelism is not as good as Java's. On top of that, Scala seems to require more debugging in testing effort which is quite problematic. One would expect that functional code should lead to fewer, easy to test lines of code. \par

An interesting point in their conclusion is that the top software developers participating in their study wrote a hybrid program, half functional and half imperative. So what would be the right balance between FP and imperative for a 'good Scala program'? \par

\subsection{ Metrics for modularization assessment of Scala and C{\#} systems }
Basavaraju Muudu et. al. \cite{DBLP:conf/icse/MudduABP13} propose a series of metrics for Scala that target modularity with respect to Functional Programming features like : referential transparency, functional purity, first order functions and also Object Oriented features such as inheritance. They give the following definition for a module: ' A module is a logical collection of related files which can be modified and tested independently. A well modularized system will have low inter-module coupling and high intra-module cohesion. It helps the designers to add new modules easily.'\cite{DBLP:conf/icse/MudduABP13}. As one can see, this definition does not imply an exact rule for discovering  modules, so the system can contain a variable number of modules, depending how one delimits a module. \par

The technique they chose to use for delimiting system modules  was Modularizing by  Inverse-Depth heuristic, which is based of the project's file directory structure; they start at the source root folder and compute for each branch the maximum depth; then starting from the bottom up, they assign a module up to a certain depth; in their study, they use inverse depth of 1 for selecting modules, arguing that these are the closest approximation to actual logical modules. We would like to try other modularization techniques: package based, SBT (Scala Build Tool)  modularization and relate them to our problem: establishing code functional-ness: would a good 'functional-ness' value for our proposed metric lead also to improved modularity?.\par

The metrics for referential transparency - function purity can be easily taken out of the modularity context and be expressed in a general manner, at the level of the whole software system; then a simple description for referential transparency would be expressed as:  \par
\begin{center}
\begin{math}
  RTI = \frac{ | \{ m \in M / Pure(m) \} | } { | M | }
\end{math}
\end{center}

Where  M - set of all functions/methods of the software and  Pure(m) - predicate that says if a function is pure or not. In this case, a value close to 1 corresponds to improved referential transparency while one close to 0 says that the property is not satisfied at all. \par

\subsection {Coordinator guidance}

\subsubsection {Problem definition}
Cum ziceam, intrebarea cheie e la ce folosesti aceste metrici.

Punerea problemei in sec. 2 nu e clar definita/motivata:
"Scala is a hybrid programming lanaguage [...] there is no mechanism in
the language that enforces a functional style [...]"

Si atunci, unde, de ce, cat ar trebui sa scrii functional?
Cand Radu Marinescu a inceput sa lucreze definind metrici pentru a detecta
probleme de proiectare, acele probleme de proiectare erau in buna parte
deja definite, dar trebuiau cuantificate si apoi detectate automat.

Care e stilul de cod bun pe care l-ar propune expertul mentionat de tine?
Cum arata un program bine scris in Scala ca limbaj hibrid?
Trebuie sa fie perfect functional sau nu?
Sigur, Meijer enumera cateva caracteristici functionale. Probabil codul
predominant functional e bun, dar nu inseamna ca acela obiectual e prost
sau are acele "code flaws" si "bad smells" de care vorbesti in concluzie.

Deci cred ca ar trebui sa te lamuresti (nu in studiul asta) cum arata
"codul ideal" pe care vrei sa-l masori. Sa si trebuiasca sa il definesti,
sa validezi prin studii ce e bun si nu, si apoi sa gasesti metrici
pentru caracterizare/detectie e prea mult.

\subsubsection {General critique}

Alex,

am remarcat ca ai facut corecturi mai ales in prima parte, si ca ai
adaugat concluzii.
Pe ansamblu insa impresia e in primul rand ca vrei sa pui punct.

Inteleg ca ai consumat timp (nu cred ca disproportionat fata de ce e
in planul de invatamant), dar e totusi putin dezamagitor pentru ca
fata de ce am citit din articolele pe care le-am gasit, si chiar
fata de ce am schitat si comentat in mesajele scrise catre tine,
regasesc destul de putin.

O sa ma refer deci la partea de *studiu bibliografic*, care la tine
incepe in sec. 4 si hai sa vedem ce aflam (exceptie: aflam in 3.2 ca
[MABP13] defineste RTI). Deci:
- vdBerg: code complexity in Miranda
- Harrison: code modularity in SML
- Ryder: metrics for Haskell
- Muddu: metrics for Scala
Dar NU aflam nici concret ce metrici se folosesc, nici ce concluzii
s-au tras pe baza lor.

Apoi vine clasificarea (apropos, ce am scris in mail despre flow-graph
si functional-related tu ai scos din context si interpretat altfel:
eu am spus ca asta ar fi in completare la bulinele tale, NU o impartire
in doua categorii!).
Ai cele 4 categorii. E bine ca le-ai extras sintetic si nu pe articol.
Dar ce sunt 'pattern-related metrics'? Si de ce ar fi aplicabile
mai mult la Haskell ? Cititorul afla foarte putin de aici, ramane mai
mult cu o intrebare.

Discutia mai detaliata o ai despre Muddu et al., dar tu descrii mai
mult structura pe module decat metrici (singura metrica e cea din 3.2).

Si aici se cam termina studiul bibliografic (mai aflam ca Li a
refactorizat ceva Haskell, dar nu ce si cum).

Facand rezumatul rezumatului, esenta e:
- o enumerare de metrici (cele 4 buline), fara a *defini* de fapt
concret aproape nicicare din ele (exceptie: RTI in 3.2)
- trei limbaje pe care altii le-au analizat (Miranda, Haskell, Scala),
in doua din cazuri, legat de modularizare.

Daca incerc sa *inteleg*, analizand lista de metrici, constat ca
ultimele 3 din 4 buline nu sunt specific functionale, si despre prima
bulina, nu aflu nimic; sunt insa si metrici functionale (enumerarea
dinainte, dar nu e clar cum le definesc -- de exemplu, in 3.2 spui
"Further study is needed on elaborating how the pure function should be
dened." desi aparent inainte ai citat o definitie clara (deci nu mi-e
clar ce e clar si ce nu e clar si de ce).

Pe de alta parte, am avut discutii repetate pe mail, tu mi-ai facut
un rezumat al lui Harrison et al., eu ti-am facut rezumatul meu, etc.
Unde se regasesc astea ?

Am *citit* in mare toate articolele pe care ti le-am recomandat, si
am *retinut* niste informatii, unele interesante, chiar daca nu toate
strict legate de intentia ta, dar totusi relevante. Insa in studiul
tau gasesc foarte putin care sa ma faca sa *aflu* si sa *inteleg*
ce au facut altii. Uite, Pankratius et al (ICSE'12) scrie:
"The study of [Harrison et al.] on SML versus C++ done 15 years ago
was not conducted with a multi-paradigm language on multicore and
unfortunately had implementations that were difficult to compare;
however, that study reports similar results to ours that subjects
need more effort to test functional code as opposed to imperative code."


E o fraza, dar aflu o informatie utila despre ce au concluzionat altii.
Desigur, e relevanta pentru studiul lor (legat de programatori), si
nu pentru tine, dar e un exemplu despre cum un articol (care NU e un
studiu bibliografic!) ma invata totusi, intr-un rand, despre alt articol.

Sunt dezamagit si pentru ca am petrecut destul de mult timp cautand,
citind (a fost placut si instructiv) dar si scriind mesaje, timp in care
poate as fi putut sa imi fac sinteze pe alta tema pe care as putea poate
sa si public. Iar primul care spune 'am lucrat destul' esti tot tu.
Ar mai fi de comentat la concluzii, dar nu ma prea simt motivat
in acest context.

Deci, nu vreau sa te imping la un perfectionism exagerat, inteleg ca
ai probabil multe de facut, am auzit de la un student zvonuri ca se
poate preda studiul si la toamna (nu o sa cercetez sau comentez),
pe scurt: ce ai scris imi spune din pacate foarte putin ca studiu
bibliografic; ori acesta e scopul sau.

cele bune,

Marius



Salut,

nu cred ca e extraordinar de mult de scris.
E bine ca ai clasificat metricile, cred ca se mai poate imbunatati,
de exemplu erau flowgraph metrics sau metricile de tip functional
(ref. transparency etc.) definite de Muddu et al.

Apoi vine intrebarea cheie la ce se folosesc si la ce vrei sa le
folosesti tu. Ryder vrea sa le coreleze cu erorile, dar e foarte
incipient (poate e mai mult in teza, desi articolul e ulterior).
van den Berg \& Broek vor tot ceva similar, dar par ca discuta in alte
articole. Harrison compara FP vs. OO, Pankratius discuta experienta
programatorilor, Muddu et al analizeaza modularitatea.

Tu vrei sa vezi cat de functional e codul, dar trebuie sa te hotarasti
ce sa faci cu informatia asta, mai ales ca Scala fiind un limbaj hibrid,
(destul de) rar se programeaza complet functional in el.
…




O ora e normal daca studiezi articolul in detaliu; e mult pe articol,
pentru ca nu tot ce gasesti e realmente relevant.
Eu as citi introducere, concluzii, rezultate experimentale, si apoi
daca prezinta interes, tehnica, metodologia etc.

Obs: As folosi mai degraba metrici legate de (procesul de) dezvoltare vs.
metrici de cod decat proces/produs, pare mai clar.

Ce as retine:
- metrici utile: numar de functii apelate / de biblioteca, raportul
dintre ele; numarul de functii definite; adancimea ierarhiei de functii
(poate si celelalte)
- diferenta in raportul L/N*
Secundar (nefiind obiectivul tau): metrici de eroare (erori / SLOC);
discutabil (depinde de mediu, si studiul e vechi: timpul de testare);
suprinzator: erori de interfata ???

Pentru concluzia "alegere in functie de gust" nu e nevoie de articole.
Partea cu numarul de modificari nu o remarcasem.

Ce as cauta rapid: daca exista elemente suplimentare/clarificari in [1]:
"The results of these preparatory studies have been reported in detail
[l, 2]".

Observatii: raritatea referintelor e un element potential problematic.
Practic, ai 2 (grupuri de) articole de acum 20 de ani (van den Berg si
Harrison), unul de acum 10 ani (Ryder et al), si putin recent (Muddu,
Kiraly). O explicatie logica e ca FP e inca insuficient de larg folosita,
iar cercetarea in metrici e puternic determinata de practica.
Pe de alta parte e bine ca Meijer a propus asta ca o tema de master.
Insa trebuie sa-ti pui problema ca metricile (si in general orice numere)
nu au impact ca atare, ci doar daca sunt utile la ceva. E usor sa scrii
articole in care sa *constati* ca ceva numere sunt diferite, si ceva de
constatat se gaseste intotdeauna (asa sunt evaluarile experimentale
superficiale). Ceva mai bine e daca poti sa *explici* numerele gasite
(aici iar e pericolul de a enunta niste teorii/pareri nefondate).
Realmente util e daca pe baza numerelor poti face ceva (la metrici:
defecte de proiectare, refactorizare, focus pe testare, etc.)
Deci probabil merita de explorat domeniul 'refactoring (to) functional'
pentru a face corelari / a gasi puncte de impact.

\subsubsection {References}

O teza mai veche despre metrici in Haskell
        $https://kar.kent.ac.uk/14117/1/SOFTWARE_MEASUREMENT_FOR.pdf$
O analiza pentru proiecte in C\#/F\#: complexitate ciclomatica, dependente
        http://fsharpforfunandprofit.com/posts/cycles-and-modularity-in-the-wild/
(eu l-am putut accesa doar via archive.org). Vezi si ulterior
        http://evelinag.com/blog/2014/06-09-comparing-dependency-networks/

O discutie despre posibile metrici intr-un limbaj declarativ
        https://sdqweb.ipd.kit.edu/publications/pdfs/kapova2010b.pdf

Ryder \& Thompson'07: Software Metrics: Measuring Haskell https://kar.kent.ac.uk/14265/1/tfp-2005-paperFinal.pdf
Muddu et al. '13: Metrics for modularization assessment of Scala and C\# systems
(daca nu ai acces via UPT la IEEEXplore ti-l trimit).

Tot pe linia de comparare OO - FP, grupul lui Harrison citat de
Berg \& Broek are un raport tehnic in care printre altele discuta
si ceva metrici:
        http://cct.brookes.ac.uk/staff/resources/rachelharrison/report3-Harrison-95.pdf
…

Mai ai de exemplu si studiul asta
        http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.261.3185
care se axeaza pe programatori dar discuta tangential si caracteristici
functionale/imperative de cod si folosirea lor.

M-ar bucura sa aud ca ai cautat activ si ai mai gasit si tu ceva relevant.


Cum ziceam, cauta pe Google Scholar sa vezi cine a mai citat pe
Ryder respectiv van den Berg (nu sunt multe). Vineri gasisem niste
articole din 2010 de la Budapesta care nu m-au atras pe moment,
dar uite o sinteza din 2013
        $http://ami.ektf.hu/uploads/papers/finalpdf/AMI_42_from29to44.pdf$
si teza de doctorat a autorului
$        http://www.tnkcs.inf.elte.hu/vedes/Kiraly_Roland_Tezisek_en.pdf$
Deasemenea cred ca merita sa mai cauti prin articolele de refactorizare
functionala (chiar daca nu o sa faci asta) pentru ca refactorizarea
se face cu un scop, de a imbunatati anumite caracteristici (masurabile),
deci implicit sunt gandite pornind de la o metrica.

\subsection {Static analysis tools for Scala}
There are already a couple of static analysis tools written for Scala : ScalaStyle, ScapeGoat, Wart remover, Linger and Scala Abide. These tools are looking possible errors that might appear in Scala code and also checks for a certain style of coding. Some of the checks they perform are: identation, illegal imports, multiple declared strings, null appearances, redundant if statements, cyclomatic complexity, unreachable catch statements, unexpected recursive definitions, unassigned variables, shadowing etc. \par

Some metrics that could turn out to be useful for our study are  the presence, absence of vars and cyclomatic complexity; ScalaStyle checks that classes and objects do not define mutable fields and that functions do not define mutable variables (VarFieldChecker, VarLocalChecker) \cite{scalastyle} and it also has a metric for cyclomatic complexity. At least from an implementation perspective it would be worthwhile to take them into account.\par

\section{Conclusions and Future work}
In this paper we've looked at the main characteristics defining functional code and discussed the existing body of work regarding metrics inside functional languages. The state-of-the-art leaves a lot of room for new investigation to detect flaws. This is will be the main purpose of our research.

The leading question of this work as presented in  section 1  is  'How can we determine the degree of code "functional-ness" '? What code patterns can we define as not being functional, how can we detect them and what can we do about them? \par

In most practical situations we cannot give binary answers on the functional-ness of a codebase. Instead we will make use of metrics to quantify this aspect. The first step will be to conduct a study on which functional elements can be measured and which not. We will need to establish the granularity of what we want to investigate: intra-method level, inter-method, class-level. Afterwards, we will design a series of metrics to measure functional properties of the program. \par

The second step will be to implement a plugin in one of the main technologies used in the Scala ecosystem to make these measurements.\par

The third step will be to apply our plugin against a substantially large codebase and evaluate its effectiveness in detecting code flaws and ``bad smells''. Our theoretical model should be complete enough to give reasons and refactoring suggestions for these when we detect them.



\newpage


\bibliography{citation} 
\bibliographystyle{alpha}

\end{document}


%Futures, actors -> better abstractions [cite needed]
%Map-reduce, -> [cite needed]
%Tools -articles 
%Cite for functional properties